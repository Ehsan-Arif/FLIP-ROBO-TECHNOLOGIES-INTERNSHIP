{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524ca250",
   "metadata": {},
   "source": [
    "# Q1. Write a python program which searches all the product under a particular product from www.amazon.in.The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ad8e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ehsan\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#Lets intall selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1a84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated chrome window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c14e376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"c127ba2215d5dbad4aef75f4965621f0\")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e31301",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.amazon.in/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08856d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want\")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element(By.ID,\"twotabsearchtextbox\")\n",
    "search_job.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.ID,\"nav-search-submit-button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba7daa",
   "metadata": {},
   "source": [
    "# Q2. In the above question, now scrape the following details of each product listed in first 3 pages of yoursearch results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching url to open each page\n",
    "urls=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,\"//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\")\n",
    "    for i in url:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_elements(By.XPATH,\"//A[@CLASS='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping details of product for which you want\n",
    "Brand_Name=[]\n",
    "Name_of_Product=[]\n",
    "Ratings=[]\n",
    "Num_of_Ratings=[]\n",
    "Price=[]\n",
    "Exchange=[]\n",
    "Date=[]\n",
    "Other_Details =[]\n",
    "Availability=[]\n",
    "for i in urls:\n",
    "\n",
    "    driver.get(i)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        brand=driver.find_element(By.XPATH,\"//table[@id='productDetails_techSpec_section_1']/tbody/tr[1]/td\")\n",
    "\n",
    "        Brand_Name.append(brand.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Brand_Name.append('-')\n",
    "        \n",
    "    try:\n",
    "\n",
    "        NOP=driver.find_element(By.XPATH,\"//table[@id='productDetails_techSpec_section_1']/tbody/tr[3]/td\")\n",
    "\n",
    "        Name_of_Product.append(NOP.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Name_of_Product.append('-')\n",
    "    \n",
    "    try:\n",
    "\n",
    "        rating=driver.find_element(By.XPATH,\"//span[@class='a-size-base a-nowrap']\")\n",
    "\n",
    "        Ratings.append(rating.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Ratings.append('-')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "\n",
    "        NOR=driver.find_element(By.XPATH,\"//span[@id='acrCustomerReviewText']\")\n",
    "\n",
    "        Num_of_Ratings.append(NOR.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Num_of_Ratings.append('-')\n",
    "        \n",
    "    try:\n",
    "\n",
    "        price=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[6]/div[4]/div[1]/div[3]/div/div/div/div/form/div/div/div/div/div[2]/div[1]/div/span/span[2]/span[2]\")\n",
    "\n",
    "        Price.append(price.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Price.append('-')\n",
    "        \n",
    "    try:\n",
    "\n",
    "        Delivery=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[6]/div[4]/div[1]/div[3]/div/div/div/div/form/div/div/div/div/div[3]/div/div[2]/div[8]/div[1]/div/div/div[2]/span/span[1]\")\n",
    "\n",
    "        Date.append(Delivery.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Date.append('-')\n",
    "        \n",
    "    \n",
    "    try:\n",
    "\n",
    "        exchange=driver.find_element(By.XPATH,\"//span[@class='a-declarative']/div/a\")\n",
    "\n",
    "        Exchange.append(exchange.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Exchange.append('-')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "\n",
    "        Detail=driver.find_element(By.XPATH,\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "\n",
    "        Other_Details.append(Detail.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Other_Details.append('-')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "\n",
    "        avail=driver.find_element(By.XPATH,\"//span[@class='a-size-base a-color-success a-text-bold']\")\n",
    "\n",
    "        Availability.append(avail.text)\n",
    "\n",
    "    except:\n",
    "\n",
    "        Availability.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62301abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'Brand_Name':Brand_Name,'Name_of_Product':Name_of_Product,\"Ratings\":Ratings,\"Num_of_Ratings\":Num_of_Ratings,\"Price\":Price,\"Exchange\":Exchange,\"Date\":Date,\"Other_Details\":Other_Details,\"Availability\":Availability})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177df031",
   "metadata": {},
   "source": [
    "# Q3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ebef1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated edge window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    " #First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://images.google.com/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want\")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b725b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "fruits=driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "fruits.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,'//*[@id=\"REsRA\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ea2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching url to open each page\n",
    "urls=[]\n",
    "url=driver.find_elements(By.XPATH,\"//a[@class='VFACy kGQAp sMi44c d0NI4c lNHeqe WGvvNb']\")\n",
    "for i in url:\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//*[@id=\"islrg\"]/div[1]/div[1]/a[1]/div[1]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i > 10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\ehsan\\Downloads\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c035231",
   "metadata": {},
   "source": [
    "# CARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca71bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://images.google.com/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4218d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want\")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "cars=driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "cars.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78430623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,'//*[@id=\"REsRA\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching url to open each page\n",
    "urls=[]\n",
    "url=driver.find_elements(By.XPATH,'/html/body')\n",
    "for i in url:\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5162c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//*[@id=\"islrg\"]/div[1]/div[52]/div[1]/a[1]/div[1]/img')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i > 10:\n",
    "        breakBy.XPATH,\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\ehsan\\Downloads\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be5d25",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c12abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://images.google.com/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want \")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "search_job=driver.find_element(By.XPATH,'//*[@id=\"REsRA\"]')\n",
    "search_job.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bedc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(20):\n",
    "\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "    \n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "\n",
    "\n",
    "\n",
    "img_urls = []\n",
    "\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "\n",
    "    source= image.get_attribute('src')\n",
    "\n",
    "    if source is not None:\n",
    "\n",
    "        if(source[0:4] == 'http'):\n",
    "\n",
    "            img_urls.append(source)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "\n",
    "    if i > 10:\n",
    "\n",
    "        breakBy.XPATH\n",
    "\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "\n",
    "    response= requests.get(img_urls[i])\n",
    "\n",
    "    file = open(r\"C:\\Users\\ehsan\\Downloads\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2745c4",
   "metadata": {},
   "source": [
    "# Guitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a39a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://images.google.com/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceed39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want\")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "guitar=driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "guitar.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc900fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,'//*[@id=\"REsRA\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "    \n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//*[@id=\"islmp\"]/div/div/div/div[1]/div/div/div/c-wiz/div/div/div/div[2]')\n",
    "\n",
    "\n",
    "\n",
    "img_urls = []\n",
    "\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "\n",
    "    source= image.get_attribute('src')\n",
    "\n",
    "    if source is not None:\n",
    "\n",
    "        if(source[0:4] == 'http'):\n",
    "\n",
    "            img_urls.append(source)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "\n",
    "    if i > 10:\n",
    "\n",
    "        break\n",
    "\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "\n",
    "    response= requests.get(img_urls[i])\n",
    "\n",
    "    file = open(r\"C:\\Users\\ehsan\\Downloads\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be806cb",
   "metadata": {},
   "source": [
    "# Cakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://images.google.com/\" # Here i am assigning the link in url name variable \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Finding the web element for search \n",
    "cakes=driver.find_element(By.XPATH,'//*[@id=\"REsRA\"]')\n",
    "cakes.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,'//*[@id=\"yDmH0d\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "    \n",
    "\n",
    "images = driver.find_elements(By.XPATH,'//*[@id=\"islrg\"]/div[1]/div[1]/a[1]/div[1]/img')\n",
    "\n",
    "\n",
    "\n",
    "img_urls = []\n",
    "\n",
    "img_data = []\n",
    "\n",
    "for image in images:\n",
    "\n",
    "    source= image.get_attribute('src')\n",
    "\n",
    "    if source is not None:\n",
    "\n",
    "        if(source[0:4] == 'http'):\n",
    "\n",
    "            img_urls.append(source)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "for i in range(len(img_urls)):\n",
    "\n",
    "    if i > 10:\n",
    "\n",
    "        break\n",
    "\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 10))\n",
    "\n",
    "    response= requests.get(img_urls[i])\n",
    "\n",
    "    file = open(r\"C:\\Users\\ehsan\\Downloads\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c30427",
   "metadata": {},
   "source": [
    "# Q4 - Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) onwww.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8794c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ba7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532a152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389d8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8581a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f37c332",
   "metadata": {},
   "source": [
    "# Q5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431db79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  'https://www.google.co.in/maps'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = input(\"Enter a location name to search:\")\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "search_job=driver.find_element(By.ID,\"searchboxinput\")\n",
    "search_job.send_keys(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.ID,\"searchbox-searchbutton\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44531ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    url_str = driver.current_url\n",
    "    print(\"URL Extracted: \", url_str)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_str)\n",
    "    if len(latitude_longitude):\n",
    "        lat_lng_list = latitude_longitude[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            latitude = lat_lng_list[0]\n",
    "            longitude = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c123c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26664172",
   "metadata": {},
   "source": [
    "# Q6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) from trak.in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b6243",
   "metadata": {},
   "source": [
    "no records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66f500",
   "metadata": {},
   "source": [
    "# Q7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated edge window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dae46e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c130b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://www.digit.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,\"//div[@class='listing_container']/ul/li[9]\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Operating_sys = []\n",
    "Display = []\n",
    "Processor = []\n",
    "Memory = []\n",
    "Body = []\n",
    "Graph_proc = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04664a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #scraping the data of operating system\n",
    "try:\n",
    "    op_sys = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "    for os in op_sys:\n",
    "        Operating_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    Operating_sys.append('-')\n",
    "\n",
    "\n",
    "#scraping data of display of the Laptop\n",
    "try:\n",
    "    display = driver.find_elements(By.XPATH,\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "    for disp in display:\n",
    "        Display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    Display.append(\"-\")\n",
    "\n",
    "\n",
    "# scraping data of processor\n",
    "try:\n",
    "    processor = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[2]/td[3]\")\n",
    "    for pro in processor:\n",
    "        Processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    Processor.append('-')\n",
    "\n",
    "\n",
    "# scraping the data of memory\n",
    "try:\n",
    "    memory = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "    for memo in memory:\n",
    "        Memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    Memory.append('-')\n",
    "\n",
    "\n",
    " # scraping data of dimensions\n",
    "try:\n",
    "    Body = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\")\n",
    "    for dim in dimension:\n",
    "        Body.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    Body.append('-')\n",
    "\n",
    "\n",
    "# scraping data of graph processor\n",
    "try:\n",
    "    graph = driver.find_elements(By.XPATH,\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "    for gra in graph:\n",
    "        Graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    Graph_proc.append('-')\n",
    "\n",
    "\n",
    "# scraping the data of price\n",
    "try:\n",
    "    price = driver.find_elements(By.XPATH,\"//td[@class='smprice']\") \n",
    "    for pri in price:\n",
    "        Price.append(pri.text)\n",
    "except NoSuchElementException:\n",
    "    Price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90567ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy.ma.mrecords as mrecords\n",
    "laptop=pd.DataFrame({'op_sys':Operating_sys,'display':Display,'processor':Processor,'memory':Memory,'Body':Body,'graph':Graph_proc,'Price':Price})\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0c1c0",
   "metadata": {},
   "source": [
    "# Q8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated edge window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653da9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Connect to web driver \n",
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9025f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://www.forbes.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element(By.XPATH,\"//div[@class='_69hVhdY4']\")\n",
    "opt_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element(By.XPATH,\"//div[@class='mpBfVZz3']\")\n",
    "opt_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8228956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get option button from the page\n",
    "opt_btn = driver.find_element(By.XPATH,'//*[@id=\"__next\"]/div[1]/header/div[1]/div/div[2]/ul/li[1]/div[2]/div[3]/ul/li[1]/a')\n",
    "opt_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e006d40",
   "metadata": {},
   "outputs": [],
   "source": [
    " # creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "Net_worth = []\n",
    "Age = []\n",
    "Citizenship = []\n",
    "Source = []\n",
    "Industry = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # scraping the data of rank of the billionaires\n",
    "    rank_tag = driver.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "    for rank in rank_tag:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    " \n",
    "    # scraping the data  of names of the billionaires\n",
    "    name_tag = driver.find_elements(By.XPATH,\"//div[@class='personName']/div\")\n",
    "    for name in name_tag:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of age of the billionaires\n",
    "    age_tag = driver.find_elements(By.XPATH,\"//div[@class='age']/div\")\n",
    "    for age in age_tag:\n",
    "        Age.append(age.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of citizenship of the billionaires\n",
    "    cit_tag = driver.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tag:\n",
    "        Citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping the data of source of income of the billionaires\n",
    "    sour_tag = driver.find_elements(By.XPATH,\"//div[@class='source']/div/div\")\n",
    "    for sour in sour_tag:\n",
    "        Source.append(sour.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of industry of the billionaires\n",
    "    ind_tag = driver.find_elements(By.XPATH,\"//div[@class='category']//div\")\n",
    "    for ind in ind_tag:\n",
    "        Industry.append(ind.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # scraping data of net_worth of billionaires\n",
    "    net_tag = driver.find_elements(By.XPATH,\"//div[@class='netWorth']/div\")\n",
    "    for net in net_tag:\n",
    "        Net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    # clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH,\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d154cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source=Source[0::2]\n",
    "len(Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make dataframe\n",
    "import pandas as pd\n",
    "forbes=pd.DataFrame({\"Rank\":Rank,\"Person_Name\":Person_Name,\"Net_worth\":Net_worth,\"Age\":Age,\"Citizenship\":Citizenship,\"Source\":Source,\"Industry\":Industry })\n",
    "forbes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4fe33",
   "metadata": {},
   "source": [
    "# Q9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated edge window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2469eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "url =\"https://www.youtube.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b02462",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(\"Search What You Want \")\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the web element for search \n",
    "search_job=driver.find_element(By.XPATH,\"//input[@id='search']\")\n",
    "search_job.send_keys(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,\"//button[@id='search-icon-legacy']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking using absolute xpath function\n",
    "search_btn=driver.find_element(By.XPATH,\"//a[@id='video-title']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 times we scroll down by 10000 in order to generate more comments\n",
    "for _ in range(1500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = []\n",
    "upvt = []\n",
    "cmttime = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt_tag = driver.find_elements(By.XPATH,\"//yt-formatted-string[@id='content-text']\")\n",
    "for i in cmt_tag:\n",
    "    cmt.append(i.text.replace(\"\\n\",''))\n",
    "\n",
    "upvt_tag = driver.find_elements(By.XPATH,\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for j in upvt_tag:\n",
    "    try: \n",
    "        upvt.append(j.text)\n",
    "    except:\n",
    "        upvt.append('-')\n",
    "cmttime_tag = driver.find_elements(By.XPATH,\"//a[contains(text(),'ago')]\")\n",
    "for k in cmttime_tag:\n",
    "    try: \n",
    "        cmttime.append(k.text)\n",
    "    except:\n",
    "        cmttime.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt=cmt[0:500]\n",
    "len(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmttime=cmttime[0:500]\n",
    "len(cmttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21898419",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmttime=cmttime[0:500]\n",
    "len(cmttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417739c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "upvt=upvt[0:500]\n",
    "len(upvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be437705",
   "metadata": {},
   "outputs": [],
   "source": [
    "upvt=upvt[1::2]\n",
    "len(upvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make dataFrame\n",
    "youtube=pd.DataFrame({\"Comments\":cmt,\" Comment upvote\":upvt,\"time\":cmttime})\n",
    "youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8144b71",
   "metadata": {},
   "source": [
    "# QQ10 : Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f49681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import all the required libraries \n",
    "import selenium # Libraries that is used to work  with selenium\n",
    "from selenium import webdriver # importing web driver module from selenium to open automated edge window\n",
    "import pandas as pd # to create dataframe\n",
    "from selenium.webdriver.common.by import By#importing inbuild class BY\n",
    "import warnings #to ignore any sort warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time# used to stop search engine for few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54191320",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\ehsan\\anaconda3\\chrome.exe\")\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fab6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://www.hostelworld.com/s?q=London, England&country=England&city=London&type=city&id=3&from=2022-11-25&to=2022-11-28&guests=2&page=1\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a71530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list & find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb586b",
   "metadata": {},
   "source": [
    "## hostel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping  hostel name\n",
    "try:\n",
    "    name = driver.find_elements(By.XPATH,'//*[@id=\"__layout\"]/div/div[2]/section/div[4]/div/div[1]/h1/div')\n",
    "    for i in name:\n",
    "        hostel_name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    hostel_name.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hostel_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7b2fe",
   "metadata": {},
   "source": [
    "## distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping distance from city centre\n",
    "try:\n",
    "    dist = driver.find_elements(By.XPATH,'//*[@id=\"__layout\"]/div/div[2]/section/div[4]/div/div[1]/div/div/button')\n",
    "    for i in dist:\n",
    "        distance.append(i.text.replace(\"Hostel -\",\"\"))\n",
    "except NoSuchElementException:\n",
    "    distance.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdeeb93",
   "metadata": {},
   "source": [
    "## dorms_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dorms = driver.find_elements(By.XPATH,'//*[@id=\"nav-prices\"]/div')\n",
    "    for i in dorms:\n",
    "        dorms_price.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    dorms_price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dorms_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0676c",
   "metadata": {},
   "source": [
    "## reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rev= driver.find_elements(By.XPATH,'//*[@id=\"nav-reviews\"]/div')\n",
    "    for i in rev:\n",
    "        reviews.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    reviews.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f59a7d",
   "metadata": {},
   "source": [
    "## rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357b982",
   "metadata": {},
   "outputs": [],
   "source": [
    " try:\n",
    "    rat = driver.find_elements(By.XPATH,'//*[@id=\"__layout\"]/div/div[2]/section/div[13]/div[2]/div/div[2]/section[2]/div/div[1]')\n",
    "    for i in rat:\n",
    "        rating.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rating.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d401bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d8cec",
   "metadata": {},
   "source": [
    " ## facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fac1 =driver.find_elements(By.XPATH,'//*[@id=\"facilities-section\"]/div/ul/li[1]/ul/li[3]')\n",
    "    fac2 =driver.find_elements(By.XPATH,'//*[@id=\"facilities-section\"]/div/ul/li[1]/ul/li[5]')\n",
    "    \n",
    "    \n",
    "    for i in fac1:\n",
    "        for j in fac2:\n",
    "            \n",
    "            facilities.append(i.text +', '+ j.text)\n",
    "except NoSuchElementException:\n",
    "     facilities.append('-')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities=facilities[0:19]\n",
    "len(facilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f8249",
   "metadata": {},
   "source": [
    "## description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching url to open each page\n",
    "urls=[]\n",
    "url=driver.find_elements(By.XPATH,'//*[@id=\"__layout\"]/div/div[2]/section/div[6]/div/div[2]/div/div/div[1]/h4')\n",
    "for i in url:\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38050b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b21ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
